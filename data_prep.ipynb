{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00000-88fc801b-cc10-4c90-a50c-59110e07f396",
    "deepnote_cell_type": "markdown",
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Parse-XML-Data\" data-toc-modified-id=\"Parse-XML-Data-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Parse XML Data</a></span></li><li><span><a href=\"#Tokenize-Text\" data-toc-modified-id=\"Tokenize-Text-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Tokenize Text</a></span></li><li><span><a href=\"#Create-Word-Embeddings-with-Wiki-Extvec\" data-toc-modified-id=\"Create-Word-Embeddings-with-Wiki-Extvec-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Create Word Embeddings with Wiki-Extvec</a></span></li><li><span><a href=\"#Create-Contextual-String-Embeddings-with-Flair\" data-toc-modified-id=\"Create-Contextual-String-Embeddings-with-Flair-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Create Contextual String Embeddings with Flair</a></span></li><li><span><a href=\"#Create-BERT-Embeddings-with-flair\" data-toc-modified-id=\"Create-BERT-Embeddings-with-flair-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Create BERT Embeddings with flair</a></span></li><li><span><a href=\"#Create-ELMo-Embeddings-with-flair\" data-toc-modified-id=\"Create-ELMo-Embeddings-with-flair-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>Create ELMo Embeddings with flair</a></span></li><li><span><a href=\"#Build-Dataset\" data-toc-modified-id=\"Build-Dataset-7\"><span class=\"toc-item-num\">7&nbsp;&nbsp;</span>Build Dataset</a></span><ul class=\"toc-item\"><li><span><a href=\"#Tagging\" data-toc-modified-id=\"Tagging-7.1\"><span class=\"toc-item-num\">7.1&nbsp;&nbsp;</span>Tagging</a></span></li><li><span><a href=\"#Split-Dataset\" data-toc-modified-id=\"Split-Dataset-7.2\"><span class=\"toc-item-num\">7.2&nbsp;&nbsp;</span>Split Dataset</a></span></li></ul></li><li><span><a href=\"#Data-compress\" data-toc-modified-id=\"Data-compress-8\"><span class=\"toc-item-num\">8&nbsp;&nbsp;</span>Data compress</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-30T07:21:57.478760Z",
     "start_time": "2019-01-30T07:21:57.474162Z"
    },
    "cell_id": "00001-ee767a82-05ff-4481-af3d-4e61466d6b44",
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "# Parse XML Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-13T14:40:51.454360Z",
     "start_time": "2019-09-13T14:40:51.263265Z"
    },
    "cell_id": "00002-a4dae8f0-c4a3-4827-807c-8f45370e6efa",
    "deepnote_cell_type": "code"
   },
   "outputs": [],
   "source": [
    "from xml.dom.minidom import parse\n",
    "import xml.dom.minidom\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "def parse(n):\n",
    "    \"\"\"\n",
    "    Parse XML corpus\n",
    "    \"\"\"\n",
    "    sentence, label = [], []\n",
    "    DOMTree = xml.dom.minidom.parse(n)\n",
    "    items = DOMTree.documentElement.getElementsByTagName('item')\n",
    "    for item in items:\n",
    "        label.append(item.getAttribute('label'))\n",
    "        sent = item.getElementsByTagName('sentence')\n",
    "        sentence.append(sent[0].childNodes[0].data)\n",
    "    return sentence, label\n",
    "\n",
    "\n",
    "trainSent, trainLabel = parse('corpus/train-corpus.xml')\n",
    "testSent, testLabel = parse('corpus/test-corpus.xml')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-30T07:23:06.567214Z",
     "start_time": "2019-01-30T07:23:06.562482Z"
    },
    "cell_id": "00003-051e1ba2-c9fb-4e51-8a69-889133a84315",
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "# Tokenize Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-13T14:41:03.578566Z",
     "start_time": "2019-09-13T14:41:01.837726Z"
    },
    "cell_id": "00004-e5a555c8-305e-4517-bd8d-6f538f5011e1",
    "deepnote_cell_type": "code"
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import text_to_word_sequence\n",
    "import numpy as np\n",
    "import pickle\n",
    "import re\n",
    "MAX_WLEN = 58\n",
    "MAX_CLEN = 23\n",
    "\n",
    "\n",
    "def delete(s):\n",
    "    \"\"\"\n",
    "    Delete parentheses\n",
    "    \"\"\"\n",
    "    return re.sub('\\((.*?)\\)', '', s)\n",
    "\n",
    "\n",
    "def cut(s):\n",
    "    \"\"\"\n",
    "    Word segmentation\n",
    "    \"\"\"\n",
    "    ws = text_to_word_sequence(s,\n",
    "                               filters='!#$&*+.%/<=>?@[\\\\]^_`{|}~\\t\\n',\n",
    "                               lower=False,\n",
    "                               split=' ')\n",
    "    for s in [',', ':', ';', '\"']:\n",
    "        sep_pm(ws, s)\n",
    "    return ws\n",
    "\n",
    "\n",
    "def sep_pm(ws, s):\n",
    "    \"\"\"\n",
    "    Separate punctuation mark: , : ; \" \n",
    "    \"\"\"\n",
    "    if s in [',', ':', ';', '\"']:\n",
    "        for i in range(len(ws)):\n",
    "            if ws[i].endswith(s) and ws[i] != s:\n",
    "                ws[i] = ws[i][:-1]\n",
    "                ws.insert(i+1, s)\n",
    "        for i in range(len(ws)):\n",
    "            if ws[i].endswith(s) and ws[i] != s:\n",
    "                ws[i] = ws[i][:-1]\n",
    "                ws.insert(i+1, s)\n",
    "    if s in ['\"']:\n",
    "        for i in range(len(ws)):\n",
    "            if ws[i].startswith(s) and ws[i] != s:\n",
    "                ws[i] = ws[i][1:]\n",
    "                ws.insert(i, s)\n",
    "        for i in range(len(ws)):\n",
    "            if ws[i].startswith(s) and ws[i] != s:\n",
    "                ws[i] = ws[i][1:]\n",
    "                ws.insert(i, s)\n",
    "\n",
    "\n",
    "def find_cp_idx(ws):\n",
    "    \"\"\"\n",
    "    Find index of causality phrases\n",
    "    \"\"\"\n",
    "    E = []\n",
    "    for n in ['1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11']:\n",
    "        e = []\n",
    "        for i in range(len(ws)):\n",
    "            if ws[i] == 'e'+n:\n",
    "                e.append(i-2*(int(n)-1))\n",
    "                for j in range(i+1, len(ws)):\n",
    "                    if ws[j] == 'e'+n and j-2 != i:\n",
    "                        e.append(j-2*int(n))\n",
    "                break\n",
    "        if e != []:\n",
    "            E.append([i for i in range(e[0], e[-1]+1)])\n",
    "    return E\n",
    "\n",
    "\n",
    "def format_sentence(ws):\n",
    "    \"\"\"\n",
    "    Remove entity tags\n",
    "    \"\"\"\n",
    "    return [i for i in ws if i not in ['e1', 'e2', 'e3', 'e4', 'e5', 'e6', 'e7', 'e8', 'e9', 'e10', 'e11']]\n",
    "\n",
    "\n",
    "train_index = []\n",
    "trainSent = [delete(i) for i in trainSent]\n",
    "trainWords = [cut(i) for i in trainSent]\n",
    "for w in trainWords:\n",
    "    train_index.append(find_cp_idx(w))\n",
    "trainWords = [format_sentence(i) for i in trainWords]\n",
    "trainChars = [[list(w) for w in s] for s in trainWords]\n",
    "\n",
    "test_index = []\n",
    "testSent = [delete(i) for i in testSent]\n",
    "testWords = [cut(i) for i in testSent]\n",
    "for w in testWords:\n",
    "    test_index.append(find_cp_idx(w))\n",
    "testWords = [format_sentence(i) for i in testWords]\n",
    "testChars = [[list(w) for w in s] for s in testWords]\n",
    "\n",
    "counts = Counter()\n",
    "for sw in trainWords+testWords:\n",
    "    counts.update(sw)\n",
    "\n",
    "vocab = sorted(counts, key=counts.get, reverse=True)\n",
    "word2index = {w: i for i, w in enumerate(vocab, 1)}\n",
    "index2word = {i: w for w, i in word2index.items()}\n",
    "\n",
    "counts = Counter()\n",
    "for sc in trainChars+testChars:\n",
    "    counts.update(sum(sc, []))\n",
    "\n",
    "c_vocab = sorted(counts, key=counts.get, reverse=True)\n",
    "char2index = {w: i for i, w in enumerate(c_vocab, 1)}\n",
    "index2char = {i: w for w, i in char2index.items()}\n",
    "\n",
    "w_filePath = 'data/index/index_w.pkl'\n",
    "with open(w_filePath, 'wb') as fp:\n",
    "    pickle.dump((word2index, index2word), fp, -1)\n",
    "\n",
    "c_filePath = 'data/index/index_c.pkl'\n",
    "with open(c_filePath, 'wb') as fp:\n",
    "    pickle.dump((char2index, index2char), fp, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-13T14:41:03.599546Z",
     "start_time": "2019-09-13T14:41:03.580711Z"
    },
    "cell_id": "00005-6e9472f1-b1d6-41f1-aa67-de8aaf286ad3",
    "deepnote_cell_type": "code"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "------ Causal Sentence Analysis ------\n",
      "\n",
      "                                           number of causality triplet\n",
      "\n",
      "1:                                                                 126\n",
      "2:                                                                  47\n",
      "3:                                                                   7\n",
      "4:                                                                   8\n",
      "5:                                                                   1\n",
      "6:                                                                   1\n",
      "7:                                                                   0\n",
      "8:                                                                   0\n",
      "9:                                                                   0\n",
      "10:                                                                  0\n",
      "11:                                                                  0\n",
      "12:                                                                  1\n",
      "total                                                              296\n"
     ]
    }
   ],
   "source": [
    "causalLabel = [l for l in testLabel if l != 'Non-Causal']\n",
    "print('\\n------ Causal Sentence Analysis ------\\n')\n",
    "print('{:<40}{:>30}\\n'.format('', 'number of causality triplet'))\n",
    "for l in ['1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12']:\n",
    "    print('{:<40}{:>30d}'.format(\n",
    "        l+':', Counter([Counter(i[12:])['e']/2 for i in causalLabel])[int(l)]))\n",
    "print('{:<40}{:>30d}'.format('total', int(\n",
    "    sum([Counter(i[12:])['e']/2 for i in causalLabel]))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00006-359d41dd-7f9e-46e2-bcd9-ebc1f08066d2",
    "deepnote_cell_type": "markdown",
    "heading_collapsed": true
   },
   "source": [
    "# Create Word Embeddings with Wiki-Extvec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-16T06:08:13.787182Z",
     "start_time": "2019-08-16T06:08:12.197528Z"
    },
    "cell_id": "00007-ef9267d1-77ce-40a1-98ab-4225c0f7c5e2",
    "deepnote_cell_type": "code",
    "hidden": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hjbec\\.conda\\envs\\krishna\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from pynlp import StanfordCoreNLP\n",
    "from nltk import WordNetLemmatizer, PorterStemmer, LancasterStemmer\n",
    "import spacy\n",
    "import math\n",
    "import h5py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import en_core_web_sm\n",
    "nlp = en_core_web_sm.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# C:\\Users\\hjbec\\stanford-corenlp-4.5.2\\stanford-corenlp-4.5.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-16T06:08:52.260980Z",
     "start_time": "2019-08-16T06:08:36.738969Z"
    },
    "cell_id": "00008-97167ae4-d3d9-4ede-bffb-b7edb550ba32",
    "deepnote_cell_type": "code",
    "hidden": true
   },
   "outputs": [],
   "source": [
    "annotators = 'lemma'\n",
    "core_nlp = StanfordCoreNLP(annotators=annotators)\n",
    "#nlp = spacy.load('en_core_web_sm')\n",
    "wnl = WordNetLemmatizer()\n",
    "porter = PorterStemmer()\n",
    "lancaster = LancasterStemmer()\n",
    "\n",
    "VOCAB_SIZE = len(word2index)+1\n",
    "EXTVEC_DIM = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en-core-web-sm==3.5.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.5.0/en_core_web_sm-3.5.0-py3-none-any.whl (12.8 MB)\n",
      "     ---------------------------------------- 12.8/12.8 MB 4.1 MB/s eta 0:00:00\n",
      "Requirement already satisfied: spacy<3.6.0,>=3.5.0 in c:\\users\\hjbec\\appdata\\roaming\\python\\python37\\site-packages (from en-core-web-sm==3.5.0) (3.5.0)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\hjbec\\.conda\\envs\\krishna\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.28.1)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\users\\hjbec\\.conda\\envs\\krishna\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.0.4)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\hjbec\\.conda\\envs\\krishna\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.0.7)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in c:\\users\\hjbec\\.conda\\envs\\krishna\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (0.9.1)\n",
      "Requirement already satisfied: pathy>=0.10.0 in c:\\users\\hjbec\\appdata\\roaming\\python\\python37\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (0.10.1)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\users\\hjbec\\.conda\\envs\\krishna\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.0.8)\n",
      "Requirement already satisfied: numpy>=1.15.0 in c:\\users\\hjbec\\.conda\\envs\\krishna\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.21.5)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in c:\\users\\hjbec\\.conda\\envs\\krishna\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (6.3.0)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\\users\\hjbec\\.conda\\envs\\krishna\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.3.0)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\hjbec\\.conda\\envs\\krishna\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.0.6)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in c:\\users\\hjbec\\.conda\\envs\\krishna\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.0.12)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in c:\\users\\hjbec\\.conda\\envs\\krishna\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.4.5)\n",
      "Requirement already satisfied: typer<0.8.0,>=0.3.0 in c:\\users\\hjbec\\.conda\\envs\\krishna\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (0.7.0)\n",
      "Requirement already satisfied: thinc<8.2.0,>=8.1.0 in c:\\users\\hjbec\\appdata\\roaming\\python\\python37\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (8.1.7)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in c:\\users\\hjbec\\.conda\\envs\\krishna\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.10.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\hjbec\\.conda\\envs\\krishna\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (21.3)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\hjbec\\.conda\\envs\\krishna\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.1.2)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\hjbec\\.conda\\envs\\krishna\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (4.64.1)\n",
      "Requirement already satisfied: setuptools in c:\\users\\hjbec\\.conda\\envs\\krishna\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (65.5.0)\n",
      "Requirement already satisfied: typing-extensions<4.5.0,>=3.7.4.1 in c:\\users\\hjbec\\.conda\\envs\\krishna\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (4.3.0)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\hjbec\\.conda\\envs\\krishna\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.0.8)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\users\\hjbec\\.conda\\envs\\krishna\\lib\\site-packages (from catalogue<2.1.0,>=2.0.6->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.8.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\hjbec\\.conda\\envs\\krishna\\lib\\site-packages (from packaging>=20.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.0.9)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\hjbec\\.conda\\envs\\krishna\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.4)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\hjbec\\.conda\\envs\\krishna\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.1.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\hjbec\\.conda\\envs\\krishna\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.26.14)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\hjbec\\.conda\\envs\\krishna\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2022.12.7)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in c:\\users\\hjbec\\.conda\\envs\\krishna\\lib\\site-packages (from thinc<8.2.0,>=8.1.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (0.7.9)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in c:\\users\\hjbec\\.conda\\envs\\krishna\\lib\\site-packages (from thinc<8.2.0,>=8.1.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (0.0.4)\n",
      "Requirement already satisfied: colorama in c:\\users\\hjbec\\.conda\\envs\\krishna\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (0.4.5)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in c:\\users\\hjbec\\.conda\\envs\\krishna\\lib\\site-packages (from typer<0.8.0,>=0.3.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (8.0.4)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\hjbec\\.conda\\envs\\krishna\\lib\\site-packages (from jinja2->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.1.1)\n",
      "Requirement already satisfied: importlib-metadata in c:\\users\\hjbec\\appdata\\roaming\\python\\python37\\site-packages (from click<9.0.0,>=7.1.1->typer<0.8.0,>=0.3.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (6.0.0)\n",
      "Installing collected packages: en-core-web-sm\n",
      "  Attempting uninstall: en-core-web-sm\n",
      "    Found existing installation: en-core-web-sm 2.3.1\n",
      "    Uninstalling en-core-web-sm-2.3.1:\n",
      "      Successfully uninstalled en-core-web-sm-2.3.1\n",
      "Successfully installed en-core-web-sm-3.5.0\n",
      "[+] Download and installation successful\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-16T06:10:36.086162Z",
     "start_time": "2019-08-16T06:10:36.073670Z"
    },
    "cell_id": "00009-62b32e0b-c003-4e92-aa1a-eeee292cdda4",
    "deepnote_cell_type": "code",
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def replace(w, d):\n",
    "    \"\"\"\n",
    "    Replace the words that are not in the dictionary\n",
    "    \"\"\"\n",
    "    r = d.get(w)\n",
    "    if r is None:\n",
    "        nw = w.lower()\n",
    "        r = d.get(nw)\n",
    "    if r is None:\n",
    "        nw = [i[0].lemma for i in core_nlp(w)][0]\n",
    "        r = d.get(nw)\n",
    "    if r is None:\n",
    "        nw = [i.lemma_ for i in nlp(w)][0]\n",
    "        r = d.get(nw)\n",
    "    if r is None:\n",
    "        nw = wnl.lemmatize(w)\n",
    "        r = d.get(nw)\n",
    "    if r is None:\n",
    "        nw = porter.stem(w)\n",
    "        r = d.get(nw)\n",
    "    if r is None:\n",
    "        nw = lancaster.stem(w)\n",
    "        r = d.get(nw)\n",
    "    if r is None:\n",
    "        nw = w[:-1]\n",
    "        r = d.get(nw)\n",
    "    if r is None:\n",
    "        nw = w[:-2]\n",
    "        r = d.get(nw)\n",
    "    return r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-16T06:04:29.900025Z",
     "start_time": "2019-08-16T06:01:40.576668Z"
    },
    "cell_id": "00010-2466fcbd-5386-4302-b009-572d28573f57",
    "deepnote_cell_type": "code",
    "hidden": true
   },
   "outputs": [],
   "source": [
    "extvec_n_symbols = 1476022\n",
    "extvec_index_dict = {}\n",
    "extvec_embedding_weights = np.empty((extvec_n_symbols, EXTVEC_DIM))\n",
    "#filePath = 'your path to /dependency-based_word_embeddings/wiki_extvec'\n",
    "filePath = r\"C:\\Users\\hjbec\\wiki_extvec\\wiki_extvec\"\n",
    "with open(filePath, encoding='utf-8') as fp:\n",
    "    index = 0\n",
    "    for l in fp:\n",
    "        l = l.split(' ')\n",
    "        word = l[0]\n",
    "        extvec_index_dict[word] = index\n",
    "        extvec_embedding_weights[index, :] = np.asarray(l[1:], dtype='float32')\n",
    "        index += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-16T06:10:42.992629Z",
     "start_time": "2019-08-16T06:10:42.874544Z"
    },
    "cell_id": "00011-0387f193-5e70-4c31-a312-b198ac57b8d0",
    "deepnote_cell_type": "code",
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Generate random embedding with same scale as extvec\n",
    "SEED = 666\n",
    "np.random.seed(SEED)\n",
    "shape = (VOCAB_SIZE, EXTVEC_DIM)\n",
    "scale = math.sqrt(3.0 / EXTVEC_DIM)\n",
    "extvec_embedding = np.random.uniform(low=-scale, high=scale, size=shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\hjbec\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\hjbec\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('omw-1.4')\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-16T06:16:54.527740Z",
     "start_time": "2019-08-16T06:16:34.487785Z"
    },
    "cell_id": "00012-566ab896-d076-48a7-b337-e8cfa8f2a255",
    "deepnote_cell_type": "code",
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14995-96.499% tokens in vocab found in Wiki-Extvec and copied to embedding.\n"
     ]
    }
   ],
   "source": [
    "# Copy from extvec weights of words that appear in index2word\n",
    "count = 0\n",
    "for i in range(1, VOCAB_SIZE):\n",
    "    w = index2word[i]\n",
    "    g = extvec_index_dict.get(w)\n",
    "    if g is None:\n",
    "        g = replace(w, extvec_index_dict)\n",
    "    if g is not None:\n",
    "        extvec_embedding[i, :] = extvec_embedding_weights[g, :]\n",
    "        count += 1\n",
    "print('{num_tokens}-{per:.3f}% tokens in vocab found in Wiki-Extvec and copied to embedding.'.format(\n",
    "    num_tokens=count, per=count/float(VOCAB_SIZE)*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-16T06:17:40.831890Z",
     "start_time": "2019-08-16T06:17:40.669462Z"
    },
    "cell_id": "00013-36e55563-3064-458e-85da-7f827273a9cc",
    "deepnote_cell_type": "code",
    "hidden": true
   },
   "outputs": [],
   "source": [
    "filePath = 'data/embedding/extvec_embedding.npy'\n",
    "np.save(open(filePath, 'wb'), extvec_embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00014-d0a2f7e7-5c26-4f8f-92d1-1c7206cc7853",
    "deepnote_cell_type": "markdown",
    "heading_collapsed": true
   },
   "source": [
    "# Create Contextual String Embeddings with Flair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-15T16:54:22.149215Z",
     "start_time": "2019-08-15T16:54:22.138565Z"
    },
    "cell_id": "00015-89580b53-6847-41f0-bcde-16e816801064",
    "deepnote_cell_type": "code",
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from flair.embeddings import FlairEmbeddings, StackedEmbeddings\n",
    "from tqdm import tqdm\n",
    "from flair.data import Sentence\n",
    "FLAIR_DIM = 4096\n",
    "\n",
    "\n",
    "def flair_cse(sw):\n",
    "    \"\"\"\n",
    "    Convert sentence to contextual string embeddings with flair\n",
    "    \"\"\"\n",
    "    #charlm_embedding_forward = FlairEmbeddings('news-forward')\n",
    "    #charlm_embedding_backward = FlairEmbeddings('your path to /news-backward-0.4.1.pt')\n",
    "    #stacked_embeddings = StackedEmbeddings(\n",
    "    #    embeddings=[charlm_embedding_forward, charlm_embedding_backward])\n",
    "    stacked_embeddings = StackedEmbeddings([FlairEmbeddings('en-forward'),\n",
    "                                        FlairEmbeddings('en-backward'),\n",
    "                                       ])\n",
    "    result = []\n",
    "    nsw = [Sentence(' '.join(i)) for i in sw]\n",
    "    for s in tqdm(nsw):\n",
    "        stacked_embeddings.embed(s)\n",
    "        result.append(np.concatenate((np.array([np.array(\n",
    "            token.embedding) for token in s]), np.zeros((MAX_WLEN-len(s), FLAIR_DIM))), axis=0))\n",
    "    return np.array(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-15T17:35:40.708248Z",
     "start_time": "2019-08-15T16:54:23.309924Z"
    },
    "cell_id": "00016-0da9be2e-57a3-40b7-a1af-f67870d024d3",
    "deepnote_cell_type": "code",
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-02-07 18:52:44,387 https://flair.informatik.hu-berlin.de/resources/embeddings/flair/news-forward-0.4.1.pt not found in cache, downloading to C:\\Users\\hjbec\\AppData\\Local\\Temp\\tmppnwavedt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 73034624/73034624 [00:02<00:00, 27326751.66B/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-02-07 18:52:47,109 copying C:\\Users\\hjbec\\AppData\\Local\\Temp\\tmppnwavedt to cache at C:\\Users\\hjbec\\.flair\\embeddings\\news-forward-0.4.1.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-02-07 18:52:47,218 removing temp file C:\\Users\\hjbec\\AppData\\Local\\Temp\\tmppnwavedt\n",
      "2023-02-07 18:52:47,511 https://flair.informatik.hu-berlin.de/resources/embeddings/flair/news-backward-0.4.1.pt not found in cache, downloading to C:\\Users\\hjbec\\AppData\\Local\\Temp\\tmpes43cbs2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 73034575/73034575 [00:02<00:00, 24941989.56B/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-02-07 18:52:50,468 copying C:\\Users\\hjbec\\AppData\\Local\\Temp\\tmpes43cbs2 to cache at C:\\Users\\hjbec\\.flair\\embeddings\\news-backward-0.4.1.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-02-07 18:52:50,579 removing temp file C:\\Users\\hjbec\\AppData\\Local\\Temp\\tmpes43cbs2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4450/4450 [32:58<00:00,  2.25it/s]\n"
     ]
    }
   ],
   "source": [
    "trainFlair = flair_cse(trainWords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-15T17:43:12.749957Z",
     "start_time": "2019-08-15T17:35:40.883766Z"
    },
    "cell_id": "00017-ba949e43-3dcf-4c1a-80d8-8e7f6b76e8f1",
    "deepnote_cell_type": "code",
    "hidden": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 786/786 [05:53<00:00,  2.22it/s]\n"
     ]
    }
   ],
   "source": [
    "testFlair = flair_cse(testWords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00018-bb91e0e3-edd2-40ad-a40d-2bd87f0f6217",
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "# Create BERT Embeddings with flair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-16T01:29:56.760032Z",
     "start_time": "2019-08-16T01:29:55.479793Z"
    },
    "cell_id": "00019-200edb80-9269-44cd-87fa-8b6679e3cb3f",
    "deepnote_cell_type": "code"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hjbec\\.conda\\envs\\krishna\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from flair.embeddings import BertEmbeddings\n",
    "from tqdm import tqdm\n",
    "from flair.data import Sentence\n",
    "BERT_DIM = 4096\n",
    "\n",
    "\n",
    "def flair_bert(sw):\n",
    "    \"\"\"\n",
    "    Convert sentence to bert embeddings with flair\n",
    "    \"\"\"\n",
    "    bert_embedding = BertEmbeddings('bert-large-cased')\n",
    "    result = []\n",
    "    nsw = [Sentence(' '.join(i)) for i in sw]\n",
    "    for s in tqdm(nsw):\n",
    "        bert_embedding.embed(s)\n",
    "        result.append(np.concatenate((np.array([np.array(\n",
    "            token.embedding) for token in s]), np.zeros((MAX_WLEN-len(s), BERT_DIM))), axis=0))\n",
    "    return np.array(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-16T01:56:05.012683Z",
     "start_time": "2019-08-16T01:30:00.593846Z"
    },
    "cell_id": "00020-d7a4f020-69d0-40c4-a0ed-6c86797ee613",
    "deepnote_cell_type": "code",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hjbec\\.conda\\envs\\krishna\\lib\\site-packages\\ipykernel_launcher.py:11: DeprecationWarning: Call to deprecated method __init__. (Use 'TransformerWordEmbeddings' for all transformer-based word embeddings) -- Deprecated since version 0.4.5.\n",
      "  # This is added back by InteractiveShellApp.init_path()\n",
      "Downloading (…)solve/main/vocab.txt: 100%|██████████| 213k/213k [00:00<00:00, 418kB/s]\n",
      "C:\\Users\\hjbec\\.conda\\envs\\krishna\\lib\\site-packages\\huggingface_hub\\file_download.py:129: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\hjbec\\.cache\\huggingface\\hub. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Downloading (…)okenizer_config.json: 100%|██████████| 29.0/29.0 [00:00<00:00, 4.84kB/s]\n",
      "Downloading (…)lve/main/config.json: 100%|██████████| 762/762 [00:00<00:00, 102kB/s]\n",
      "Downloading (…)\"pytorch_model.bin\";: 100%|██████████| 1.34G/1.34G [08:28<00:00, 2.63MB/s]\n",
      "Some weights of the model checkpoint at bert-large-cased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "100%|██████████| 4450/4450 [08:06<00:00,  9.15it/s]\n"
     ]
    }
   ],
   "source": [
    "trainBERT = flair_bert(trainWords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-16T02:06:33.102788Z",
     "start_time": "2019-08-16T02:01:47.330643Z"
    },
    "cell_id": "00021-4a0338d5-2d8e-473e-872e-de838454d568",
    "deepnote_cell_type": "code"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hjbec\\.conda\\envs\\krishna\\lib\\site-packages\\ipykernel_launcher.py:11: DeprecationWarning: Call to deprecated method __init__. (Use 'TransformerWordEmbeddings' for all transformer-based word embeddings) -- Deprecated since version 0.4.5.\n",
      "  # This is added back by InteractiveShellApp.init_path()\n",
      "Some weights of the model checkpoint at bert-large-cased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "100%|██████████| 786/786 [01:25<00:00,  9.14it/s]\n"
     ]
    }
   ],
   "source": [
    "testBERT = flair_bert(testWords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00022-32cef4bb-9526-483e-bebd-2ce4fea5f6ac",
    "deepnote_cell_type": "markdown",
    "heading_collapsed": true
   },
   "source": [
    "# Create ELMo Embeddings with flair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-15T18:51:30.753766Z",
     "start_time": "2019-08-15T18:51:29.128995Z"
    },
    "cell_id": "00023-e62f704f-4e82-4903-bb64-ccdf8f5611a8",
    "deepnote_cell_type": "code",
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from flair.embeddings import ELMoEmbeddings\n",
    "from tqdm import tqdm\n",
    "from flair.data import Sentence\n",
    "ELMO_DIM = 3072\n",
    "\n",
    "\n",
    "def flair_elmo(sw):\n",
    "    \"\"\"\n",
    "    Convert sentence to ELMo embeddings with flair\n",
    "    \"\"\"\n",
    "    elmo_embedding = ELMoEmbeddings('original')\n",
    "    result = []\n",
    "    nsw = [Sentence(' '.join(i)) for i in sw]\n",
    "    for s in tqdm(nsw):\n",
    "        elmo_embedding.embed(s)\n",
    "        result.append(np.concatenate((np.array([np.array(\n",
    "            token.embedding) for token in s]), np.zeros((MAX_WLEN-len(s), ELMO_DIM))), axis=0))\n",
    "    return np.array(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-15T19:27:12.541378Z",
     "start_time": "2019-08-15T18:51:37.530066Z"
    },
    "cell_id": "00024-9abe9133-1098-43fc-8c4e-c61db98dd825",
    "deepnote_cell_type": "code",
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using AllenNLP requires the python packages Spacy, Pytorch and Numpy to be installed. Please see https://github.com/allenai/allennlp for installation instructions.\n",
      "2023-02-09 19:50:48,309 ----------------------------------------------------------------------------------------------------\n",
      "2023-02-09 19:50:48,310 ATTENTION! The library \"allennlp\" is not installed!\n",
      "2023-02-09 19:50:48,310 To use ELMoEmbeddings, please first install with \"pip install allennlp==0.9.0\"\n",
      "2023-02-09 19:50:48,311 ----------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "ename": "UnboundLocalError",
     "evalue": "local variable 'allennlp' referenced before assignment",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_27084\\1725445783.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtrainELMo\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mflair_elmo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrainWords\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_27084\\684878999.py\u001b[0m in \u001b[0;36mflair_elmo\u001b[1;34m(sw)\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[0mConvert\u001b[0m \u001b[0msentence\u001b[0m \u001b[0mto\u001b[0m \u001b[0mELMo\u001b[0m \u001b[0membeddings\u001b[0m \u001b[1;32mwith\u001b[0m \u001b[0mflair\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m     \"\"\"\n\u001b[1;32m---> 11\u001b[1;33m     \u001b[0melmo_embedding\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mELMoEmbeddings\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'pubmed'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m     \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m     \u001b[0mnsw\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mSentence\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m' '\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msw\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\krishna\\lib\\site-packages\\flair\\embeddings\\token.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, model, options_file, weight_file, embedding_mode)\u001b[0m\n\u001b[0;32m   1357\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0moptions_file\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mweight_file\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1358\u001b[0m             \u001b[1;31m# the default model for ELMo is the 'original' model, which is very large\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1359\u001b[1;33m             \u001b[0moptions_file\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mallennlp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcommands\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0melmo\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDEFAULT_OPTIONS_FILE\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1360\u001b[0m             \u001b[0mweight_file\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mallennlp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcommands\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0melmo\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDEFAULT_WEIGHT_FILE\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1361\u001b[0m             \u001b[1;31m# alternatively, a small, medium or portuguese model can be selected by passing the appropriate mode name\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mUnboundLocalError\u001b[0m: local variable 'allennlp' referenced before assignment"
     ]
    }
   ],
   "source": [
    "trainELMo = flair_elmo(trainWords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-15T19:33:23.864448Z",
     "start_time": "2019-08-15T19:27:13.015289Z"
    },
    "cell_id": "00025-0bbc2f79-e864-459a-83f8-6767e56b4b01",
    "deepnote_cell_type": "code",
    "hidden": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 786/786 [04:34<00:00,  2.14it/s]\n"
     ]
    }
   ],
   "source": [
    "testELMo = flair_elmo(testWords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00026-ba4e6081-2d4e-4e45-ac9f-55662268d19b",
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "# Build Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00027-a50e6fb3-421b-4674-aec8-4cf5e2d777b3",
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "## Tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-13T14:41:17.470972Z",
     "start_time": "2019-09-13T14:41:17.452310Z"
    },
    "cell_id": "00028-eae31611-8f10-46df-bdc0-cb6b867f0b04",
    "deepnote_cell_type": "code"
   },
   "outputs": [],
   "source": [
    "def find_cp(l):\n",
    "    \"\"\"\n",
    "    Find index of cause and effect from label\n",
    "    \"\"\"\n",
    "    p = '\\((e.*?)\\)'\n",
    "    t = re.findall(p, l)\n",
    "    t = [i.split(',') for i in t]\n",
    "    c = [i[0] for i in t]\n",
    "    e = [i[-1] for i in t]\n",
    "    return c, e\n",
    "\n",
    "\n",
    "def tagging(s, n, w):\n",
    "    \"\"\"\n",
    "    Tagging\n",
    "    \"\"\"\n",
    "    for i in range(len(w)):\n",
    "        if i == 0:\n",
    "            s[w[i]] = n\n",
    "        else:\n",
    "            s[w[i]] = n+1\n",
    "    return s\n",
    "\n",
    "\n",
    "def word2label(s, idx, l):\n",
    "    \"\"\"\n",
    "    Convert sentence to label sequence\n",
    "    :Tagging Scheme\n",
    "    ---7 Tag---\n",
    "    0: O = Other\n",
    "    1: B-C = Cause Begin\n",
    "    2: I-C = Cause Inside\n",
    "    3: B-E = Effect Begin\n",
    "    4: I-E = Effect Inside\n",
    "    5: B-CE = Cause|Effect Begin\n",
    "    6: I-CE = Cause|Effect Inside\n",
    "    \"\"\"\n",
    "    r = [0]*len(s)\n",
    "    if l == 'Non-Causal':\n",
    "        return r\n",
    "    else:\n",
    "        c, e = find_cp(l)\n",
    "        element = set(c+e)\n",
    "        cause, cause_and_effect, effect = [], [], []\n",
    "        for i in element:\n",
    "            if i not in e:\n",
    "                cause.append(i)\n",
    "            if i in c and i in e:\n",
    "                cause_and_effect.append(i)\n",
    "            if i not in c and i in e:\n",
    "                effect.append(i)\n",
    "        for e in element:\n",
    "            if e in cause:\n",
    "                r = tagging(r, 1, idx[int(e[1:])-1])\n",
    "            if e in effect:\n",
    "                r = tagging(r, 3, idx[int(e[1:])-1])\n",
    "            if e in cause_and_effect:\n",
    "                r = tagging(r, 5, idx[int(e[1:])-1])\n",
    "    return r\n",
    "\n",
    "\n",
    "train_labelSeq = [word2label(\n",
    "    trainWords[i], train_index[i], trainLabel[i]) for i in range(len(trainWords))]\n",
    "test_labelSeq = [word2label(testWords[i], test_index[i], testLabel[i])\n",
    "                 for i in range(len(testWords))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-13T14:41:19.108721Z",
     "start_time": "2019-09-13T14:41:18.211470Z"
    },
    "cell_id": "00029-d49e1bef-c798-42a2-abeb-58f37cc6f2c2",
    "deepnote_cell_type": "code"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "------ Corpous Tag Analysis ------\n",
      "\n",
      "                                     number of tag                    percentage\n",
      "\n",
      "O:                                           78440                        92.57%\n",
      "B-C:                                          1544                        1.822%\n",
      "I-C:                                          1650                        1.947%\n",
      "B-E:                                          1506                        1.777%\n",
      "I-E:                                          1460                        1.723%\n",
      "B-CE:                                           64                        0.075%\n",
      "I-CE:                                           71                        0.083%\n",
      "total                                        84735                             1\n"
     ]
    }
   ],
   "source": [
    "labelSeq = sum(train_labelSeq+test_labelSeq, [])\n",
    "tagAll = ['O', 'B-C', 'I-C', 'B-E', 'I-E', 'B-CE', 'I-CE']\n",
    "print('\\n------ Corpous Tag Analysis ------\\n')\n",
    "print('{:<20}{:>30}{:>30}\\n'.format('', 'number of tag', 'percentage'))\n",
    "for i in range(len(tagAll)):\n",
    "    print('{:<20}{:>30d}{:>30}'.format(tagAll[i]+':', Counter(labelSeq)[i], str(Counter(\n",
    "        labelSeq)[i]/sum(Counter(labelSeq)[i] for i in range(len(tagAll)))*100)[:5]+'%'))\n",
    "print('{:<20}{:>30d}{:>30d}'.format('total', sum(\n",
    "    Counter(labelSeq)[i] for i in range(len(tagAll))), 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-13T14:41:21.034482Z",
     "start_time": "2019-09-13T14:41:20.399804Z"
    },
    "cell_id": "00030-8dc7f86d-c250-47d3-9161-2d7f4205a31c",
    "deepnote_cell_type": "code"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "------ Corpous Tag Analysis ------\n",
      "\n",
      "                                     number of tag                    percentage\n",
      "\n",
      "O:                                           66614                        92.58%\n",
      "B-C:                                          1308                        1.817%\n",
      "I-C:                                          1421                        1.974%\n",
      "B-E:                                          1268                        1.762%\n",
      "I-E:                                          1230                        1.709%\n",
      "B-CE:                                           55                        0.076%\n",
      "I-CE:                                           55                        0.076%\n",
      "total                                        71951                             1\n"
     ]
    }
   ],
   "source": [
    "labelSeq = sum(train_labelSeq, [])\n",
    "tagAll = ['O', 'B-C', 'I-C', 'B-E', 'I-E', 'B-CE', 'I-CE']\n",
    "print('\\n------ Corpous Tag Analysis ------\\n')\n",
    "print('{:<20}{:>30}{:>30}\\n'.format('', 'number of tag', 'percentage'))\n",
    "for i in range(len(tagAll)):\n",
    "    print('{:<20}{:>30d}{:>30}'.format(tagAll[i]+':', Counter(labelSeq)[i], str(Counter(\n",
    "        labelSeq)[i]/sum(Counter(labelSeq)[i] for i in range(len(tagAll)))*100)[:5]+'%'))\n",
    "print('{:<20}{:>30d}{:>30d}'.format('total', sum(\n",
    "    Counter(labelSeq)[i] for i in range(len(tagAll))), 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-12T12:27:52.944230Z",
     "start_time": "2019-09-12T12:27:52.866713Z"
    },
    "cell_id": "00033-71791505-975e-49df-beb3-5b62f1d12024",
    "deepnote_cell_type": "code"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "------ Corpous Tag Analysis ------\n",
      "\n",
      "                                     number of tag                    percentage\n",
      "\n",
      "O:                                           11826                        92.50%\n",
      "B-C:                                           236                        1.846%\n",
      "I-C:                                           229                        1.791%\n",
      "B-E:                                           238                        1.861%\n",
      "I-E:                                           230                        1.799%\n",
      "B-CE:                                            9                        0.070%\n",
      "I-CE:                                           16                        0.125%\n",
      "total                                        12784                             1\n"
     ]
    }
   ],
   "source": [
    "labelSeq = sum(test_labelSeq, [])\n",
    "tagAll = ['O', 'B-C', 'I-C', 'B-E', 'I-E', 'B-CE', 'I-CE']\n",
    "print('\\n------ Corpous Tag Analysis ------\\n')\n",
    "print('{:<20}{:>30}{:>30}\\n'.format('', 'number of tag', 'percentage'))\n",
    "for i in range(len(tagAll)):\n",
    "    print('{:<20}{:>30d}{:>30}'.format(tagAll[i]+':', Counter(labelSeq)[i], str(Counter(\n",
    "        labelSeq)[i]/sum(Counter(labelSeq)[i] for i in range(len(tagAll)))*100)[:5]+'%'))\n",
    "print('{:<20}{:>30d}{:>30d}'.format('total', sum(\n",
    "    Counter(labelSeq)[i] for i in range(len(tagAll))), 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00046-2c7d0c45-f3e5-4e15-ad46-65310c76d20d",
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "## Split Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-08T08:20:16.584544Z",
     "start_time": "2019-09-08T08:20:15.971640Z"
    },
    "cell_id": "00047-2ebc934a-97ab-49c2-92ea-5bf26c0549f8",
    "deepnote_cell_type": "code"
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'pad_sequences' from 'keras.preprocessing.sequence' (C:\\Users\\hjbec\\.conda\\envs\\krishna\\lib\\site-packages\\keras\\preprocessing\\sequence.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_24236\\2890827830.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpreprocessing\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msequence\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpad_sequences\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel_selection\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel_selection\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mKFold\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mh5py\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mSEED\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m666\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'pad_sequences' from 'keras.preprocessing.sequence' (C:\\Users\\hjbec\\.conda\\envs\\krishna\\lib\\site-packages\\keras\\preprocessing\\sequence.py)"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold\n",
    "import h5py\n",
    "SEED = 666\n",
    "\n",
    "\n",
    "def char_data(cw):\n",
    "    result = []\n",
    "    for s in cw:\n",
    "        result.append(np.concatenate((pad_sequences([[char2index[c] for c in w] for w in s], maxlen=MAX_CLEN, padding='post', truncating='post'),\n",
    "                                      np.zeros((MAX_WLEN-len(s), MAX_CLEN))), axis=0))\n",
    "    return np.array(result)\n",
    "\n",
    "\n",
    "trainCrray = char_data(trainChars)\n",
    "\n",
    "testCrray = char_data(testChars)\n",
    "\n",
    "trainSeq = [[word2index[cw] for cw in s] for s in trainWords]\n",
    "trainWrray = pad_sequences(trainSeq, maxlen=MAX_WLEN,\n",
    "                           padding='post', truncating='post')\n",
    "\n",
    "testSeq = [[word2index[cw] for cw in s] for s in testWords]\n",
    "testWrray = pad_sequences(testSeq, maxlen=MAX_WLEN,\n",
    "                          padding='post', truncating='post')\n",
    "\n",
    "train_labelSeq = pad_sequences(\n",
    "    train_labelSeq, maxlen=MAX_WLEN, padding='post', truncating='post')\n",
    "train_labelData = train_labelSeq.reshape(len(trainWords), MAX_WLEN, 1)\n",
    "\n",
    "test_labelSeq = pad_sequences(\n",
    "    test_labelSeq, maxlen=MAX_WLEN, padding='post', truncating='post')\n",
    "test_labelData = test_labelSeq.reshape(len(testWords), MAX_WLEN, 1)\n",
    "\n",
    "xTrain, _, yTrain, _ = train_test_split(\n",
    "    trainWrray, train_labelData, test_size=0., random_state=SEED)\n",
    "xTest, _, yTest, _ = train_test_split(\n",
    "    testWrray, test_labelData, test_size=0., random_state=SEED)\n",
    "xTrain_c = train_test_split(trainCrray, test_size=0., random_state=SEED)[0]\n",
    "xTest_c = train_test_split(testCrray, test_size=0., random_state=SEED)[0]\n",
    "xTrain_flair = train_test_split(trainFlair, test_size=0., random_state=SEED)[0]\n",
    "xTest_flair = train_test_split(testFlair, test_size=0., random_state=SEED)[0]\n",
    "xTrain_bert = train_test_split(trainBERT, test_size=0., random_state=SEED)[0]\n",
    "xTest_bert = train_test_split(testBERT, test_size=0., random_state=SEED)[0]\n",
    "xTrain_elmo = train_test_split(trainELMo, test_size=0., random_state=SEED)[0]\n",
    "xTest_elmo = train_test_split(testELMo, test_size=0., random_state=SEED)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-10T13:45:43.715341Z",
     "start_time": "2019-09-10T13:45:43.708724Z"
    },
    "cell_id": "00049-3d69c1c1-6afd-4a34-8902-b761010aaca5",
    "deepnote_cell_type": "code"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((4501, 58),\n",
       " (4501, 58, 1),\n",
       " (786, 58),\n",
       " (786, 58, 1),\n",
       " (4501, 58, 23),\n",
       " (786, 58, 23),\n",
       " (4501, 58, 4096),\n",
       " (786, 58, 4096))"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xTrain.shape, yTrain.shape, xTest.shape, yTest.shape, xTrain_c.shape, xTest_c.shape, xTrain_flair.shape, xTest_flair.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-15T17:49:37.076801Z",
     "start_time": "2019-08-15T17:49:10.319158Z"
    },
    "cell_id": "00058-b0f160aa-a51c-43ec-81fb-8c28fe5f402b",
    "deepnote_cell_type": "code"
   },
   "outputs": [],
   "source": [
    "os.makedirs('data/train/', exist_ok=True)\n",
    "h5f = h5py.File('data/train/train.h5', 'w')\n",
    "h5f.create_dataset('xTrain', data=xTrain)\n",
    "h5f.create_dataset('xTrain_c', data=xTrain_c)\n",
    "h5f.create_dataset('yTrain', data=yTrain)\n",
    "h5f.close()\n",
    "\n",
    "os.makedirs('data/test/', exist_ok=True)\n",
    "h5f = h5py.File('data/test/test.h5', 'w')\n",
    "h5f.create_dataset('xTest', data=xTest)\n",
    "h5f.create_dataset('xTest_c', data=xTest_c)\n",
    "h5f.create_dataset('yTest', data=yTest)\n",
    "h5f.close()\n",
    "\n",
    "os.makedirs('data/embedding/', exist_ok=True)\n",
    "h5f = h5py.File('data/embedding/flair.h5', 'w')\n",
    "h5f.create_dataset('xTrain_flair', data=xTrain_flair)\n",
    "h5f.create_dataset('xTest_flair', data=xTest_flair)\n",
    "h5f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-16T02:14:15.161040Z",
     "start_time": "2019-08-16T02:13:44.930671Z"
    },
    "cell_id": "00059-651ebd8b-4ad9-47d9-beab-49fd96478d42",
    "deepnote_cell_type": "code"
   },
   "outputs": [],
   "source": [
    "h5f = h5py.File('data/embedding/bert.h5', 'w')\n",
    "h5f.create_dataset('xTrain_bert', data=xTrain_bert)\n",
    "h5f.create_dataset('xTest_bert', data=xTest_bert)\n",
    "h5f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-15T19:34:08.103928Z",
     "start_time": "2019-08-15T19:33:47.434196Z"
    },
    "cell_id": "00060-45c8548d-ba2e-4291-b879-74d4ce5cf2c3",
    "deepnote_cell_type": "code"
   },
   "outputs": [],
   "source": [
    "h5f = h5py.File('data/embedding/elmo.h5', 'w')\n",
    "h5f.create_dataset('xTrain_elmo', data=xTrain_elmo)\n",
    "h5f.create_dataset('xTest_elmo', data=xTest_elmo)\n",
    "h5f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from keras_preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold\n",
    "import h5py\n",
    "SEED = 666"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def char_data(cw):\n",
    "    result = []\n",
    "    for s in cw:\n",
    "        result.append(np.concatenate((pad_sequences([[char2index[c] for c in w] for w in s], maxlen=MAX_CLEN, padding='post', truncating='post'),\n",
    "                                      np.zeros((MAX_WLEN-len(s), MAX_CLEN))), axis=0))\n",
    "    return np.array(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainCrray = char_data(trainChars)\n",
    "\n",
    "testCrray = char_data(testChars)\n",
    "\n",
    "trainSeq = [[word2index[cw] for cw in s] for s in trainWords]\n",
    "trainWrray = pad_sequences(trainSeq, maxlen=MAX_WLEN,\n",
    "                           padding='post', truncating='post')\n",
    "\n",
    "testSeq = [[word2index[cw] for cw in s] for s in testWords]\n",
    "testWrray = pad_sequences(testSeq, maxlen=MAX_WLEN,\n",
    "                          padding='post', truncating='post')\n",
    "\n",
    "train_labelSeq = pad_sequences(\n",
    "    train_labelSeq, maxlen=MAX_WLEN, padding='post', truncating='post')\n",
    "train_labelData = train_labelSeq.reshape(len(trainWords), MAX_WLEN, 1)\n",
    "\n",
    "test_labelSeq = pad_sequences(\n",
    "    test_labelSeq, maxlen=MAX_WLEN, padding='post', truncating='post')\n",
    "test_labelData = test_labelSeq.reshape(len(testWords), MAX_WLEN, 1)\n",
    "\n",
    "xTrain, _, yTrain, _ = train_test_split(\n",
    "    trainWrray, train_labelData, test_size=0.0000001, random_state=SEED)\n",
    "xTest, _, yTest, _ = train_test_split(\n",
    "    testWrray, test_labelData, test_size=0.0000001, random_state=SEED)\n",
    "xTrain_c = train_test_split(trainCrray, test_size=0.0000001, random_state=SEED)[0]\n",
    "xTest_c = train_test_split(testCrray, test_size=0.0000001, random_state=SEED)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((4449, 58),\n",
       " (4449, 58, 1),\n",
       " (785, 58),\n",
       " (785, 58, 1),\n",
       " (4449, 58, 23),\n",
       " (785, 58, 23),\n",
       " (4449, 58, 4096),\n",
       " (785, 58, 4096))"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xTrain.shape, yTrain.shape, xTest.shape, yTest.shape, xTrain_c.shape, xTest_c.shape, xTrain_flair.shape, xTest_flair.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "############ Flair\n",
    "xTrain_flair = train_test_split(trainFlair, test_size=0.0000001, random_state=SEED)[0]\n",
    "xTest_flair = train_test_split(testFlair, test_size=0.0000001, random_state=SEED)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "############ Bert\n",
    "xTrain_bert = train_test_split(trainBERT, test_size=0.0000001, random_state=SEED)[0]\n",
    "xTest_bert = train_test_split(testBERT, test_size=0.0000001, random_state=SEED)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############ pynlp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00061-e5d91ae5-a48a-47a8-9847-03f100cf6e14",
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "# Data compress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-11T02:07:06.547811Z",
     "start_time": "2019-09-11T02:07:06.542672Z"
    },
    "cell_id": "00062-2d65ba8c-91ff-4803-8c60-8f66aa150292",
    "deepnote_cell_type": "code"
   },
   "outputs": [],
   "source": [
    "import zlib\n",
    "\n",
    "def compress(infile, dst, level=9):\n",
    "    infile = open(infile, 'rb')\n",
    "    dst = open(dst, 'wb')\n",
    "    compress = zlib.compressobj(level)\n",
    "    data = infile.read(1024)\n",
    "    while data:\n",
    "        dst.write(compress.compress(data))\n",
    "        data = infile.read(1024)\n",
    "    dst.write(compress.flush())\n",
    "\n",
    "\n",
    "def decompress(infile, dst):\n",
    "    infile = open(infile, 'rb')\n",
    "    dst = open(dst, 'wb')\n",
    "    decompress = zlib.decompressobj()\n",
    "    data = infile.read(1024)\n",
    "    while data:\n",
    "        dst.write(decompress.decompress(data))\n",
    "        data = infile.read(1024)\n",
    "    dst.write(decompress.flush())\n",
    "    \n",
    "#compress('data/embedding/bert.h5',\n",
    "#         'data/embedding/c_bert.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-11T02:07:14.085105Z",
     "start_time": "2019-09-11T02:07:07.217458Z"
    },
    "cell_id": "00063-77595f94-e17e-4e05-bd02-45c3caf79270",
    "deepnote_cell_type": "code"
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'data/embedding/c_bert.h5'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_27084\\1282240515.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdecompress\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'data/embedding/c_bert.h5'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'data/embedding/bert.h5'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_27084\\3936431739.py\u001b[0m in \u001b[0;36mdecompress\u001b[1;34m(infile, dst)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mdecompress\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minfile\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdst\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m     \u001b[0minfile\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minfile\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'rb'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m     \u001b[0mdst\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdst\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'wb'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m     \u001b[0mdecompress\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mzlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecompressobj\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data/embedding/c_bert.h5'"
     ]
    }
   ],
   "source": [
    "decompress('data/embedding/c_bert.h5', 'data/embedding/bert.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-06T08:01:43.611348Z",
     "start_time": "2019-09-06T08:01:19.991285Z"
    },
    "cell_id": "00064-32912954-885d-4972-8871-d6ebad5dc2d0",
    "deepnote_cell_type": "code"
   },
   "outputs": [],
   "source": [
    "decompress('data/embedding/c_elmo.h5', 'data/embedding/elmo.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-16T10:36:50.247301Z",
     "start_time": "2019-08-16T10:36:10.674948Z"
    },
    "cell_id": "00065-c7c0ddf1-2d92-4576-8f45-00ec342ca3f7",
    "deepnote_cell_type": "code"
   },
   "outputs": [],
   "source": [
    "decompress('data/embedding/c_flair.h5', 'data/embedding/flair.h5')"
   ]
  }
 ],
 "metadata": {
  "deepnote_execution_queue": [],
  "deepnote_notebook_id": "43b5dbd4-b093-41c1-8cb0-b0188e5a28b2",
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "384px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
